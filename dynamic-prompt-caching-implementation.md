# Dynamic Prompt Caching Implementation

This document outlines the implementation of the Anthropic API prompt caching feature in the Persona Character Simulation application.

## Overview

The prompt caching feature optimizes token usage by allowing Anthropic's Claude API to cache and reuse certain parts of prompts across requests. This implementation focuses on caching the static parts of character profiles while allowing dynamic content to change.

## Key Components

1. **PromptCacheManager** (prompt-cache-manager.js)
   - Manages the caching of static prompt components
   - Provides methods for generating cache control parameters
   - Implements test functionality to verify cache formats

2. **AnthropicChatClient Integration** (anthropic-chat-client.js)
   - Splits character profiles into static and dynamic sections
   - Integrates with PromptCacheManager to use cached prompts
   - Handles tracking of cache statistics

3. **Server Endpoints** (server.js)
   - Provides API endpoints for testing different cache control formats
   - Supports refreshing cached prompts
   - Enables cache statistics retrieval

## Implementation Details

### API Request Format

The implementation uses Anthropic's API format with system as a top-level parameter and cache_control in the content array:

```javascript
// Example API request with system and cache control
{
  model: "claude-3-7-sonnet-20250219",
  max_tokens: 2048,
  system: "The system prompt goes here", // System as top-level parameter
  messages: [
    { 
      role: 'user',
      content: [
        { type: 'text', text: 'User message content' },
        { 
          type: 'cache_control',
          breakpoints: [0] // Cache from beginning of the prompt
        }
      ]
    }
  ]
}
```

For using an existing cache, we use the ephemeral type:

```javascript
{ 
  type: 'cache_control',
  cache_type: 'ephemeral'
}
```

> **Important**: The Anthropic API expects `system` as a top-level parameter, not as a message with role 'system' in the messages array.

### Split Prompting Approach

1. **Static Template**:
   - Contains format instructions, symbolic notation guide
   - Meets minimum token requirements (1024 tokens) for caching
   - Generated by `PromptCacheManager.generateStaticTemplate()`

2. **Dynamic Content**:
   - Character-specific details
   - Memory context
   - Recent conversation history

### API Version

The implementation supports testing with different API versions:
- Default: 2023-06-01
- Alternative: 2023-12-01

## Debugging Tools

1. **Test Endpoint**: `/api/cache/test/:sessionId`
   - Tests various cache control formats
   - Supports specifying API version

2. **Refresh Endpoint**: `/api/cache/refresh/:sessionId`
   - Forces cache refresh
   - Supports specifying API version

3. **Statistics Endpoint**: `/api/cache/stats/:sessionId`
   - Retrieves cache hit/miss statistics
   - Shows token savings estimates

## Usage Notes

1. Cache creation requires a minimum of 1024 tokens for the Claude 3.7 Sonnet model.
2. The first request with a new cache involves a TTL setting.
3. Subsequent requests use the "ephemeral" type to reference the cache.
4. Cache is automatically refreshed after 300 seconds (5 minutes) minimum.

## Common Issues & Solutions

1. **"Extra inputs are not permitted" Error**:
   - This error may occur if the API version doesn't support the cache_control format used
   - Try using the newer API version (2023-12-01)
   - Ensure the cache_control block is correctly placed in the user's content array

2. **"Unexpected role 'system'" Error**:
   - This error occurs when 'system' is used as a role in the messages array
   - The Anthropic API expects system content as a top-level parameter named `system`
   - Never include `{ role: 'system', content: '...' }` in the messages array
   - Instead use `system: "System content"` at the top level of the request

3. **No Cache Creation Tokens**:
   - If no cache_creation_input_tokens appear in the response, the cache wasn't created
   - Check that the static template exceeds 1024 tokens
   - Verify API key has access to the caching feature

4. **Unexpected Token Usage**:
   - If token usage is higher than expected, verify the cache is being used
   - Check cache hit rates in the statistics endpoint
   - Try refreshing the cache

## Further Optimizations

1. Experiment with multiple breakpoints for more efficient caching
2. Consider caching different segments of the character profile separately
3. Monitor token usage metrics to identify additional caching opportunities